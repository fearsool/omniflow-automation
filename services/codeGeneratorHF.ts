/**
 * CODE GENERATOR - HuggingFace Native
 * ‚úÖ 0 maliyet (Free HF API + Ollama local)
 * ‚úÖ Python/Node.js/Docker/GitHub Actions
 * ‚úÖ Exponential backoff + 3x retry
 * ‚úÖ Timeout protection
 * ‚úÖ SQLite persistence
 */

import { SystemBlueprint, WorkflowNode, NodeType } from '../types';

export type ExportFormat = 'python' | 'nodejs' | 'github-action' | 'dockerfile';

interface GeneratedCode {
  filename: string;
  content: string;
  language: string;
}

// ==================== PYTHON GENERATOR (HuggingFace) ====================

export const generatePythonScript = (blueprint: SystemBlueprint): GeneratedCode[] => {
  const mainScript = `#!/usr/bin/env python3
"""
${blueprint.name}
Auto-generated by OmniFlow Factory - HuggingFace Native
‚úÖ 0 maliyetli otomasyon (Free HF API + Ollama)
"""

import os
import json
import time
import sys
import sqlite3
from datetime import datetime
import requests
from typing import Dict, Any, Optional

# ================== CONFIGURATION ==================

# HuggingFace kullan (free tier) veya Ollama local model
USE_OLLAMA = os.getenv('USE_OLLAMA', 'false').lower() == 'true'
HF_TOKEN = os.getenv('HUGGINGFACE_TOKEN', '')
OLLAMA_URL = os.getenv('OLLAMA_URL', 'http://localhost:11434')

# Model se√ßimi
HF_MODELS = {
    'text': 'mistralai/Mistral-7B-Instruct-v0.2',
    'analysis': 'meta-llama/Llama-2-7b-chat-hf',
    'research': 'bigcode/starcoder',
}

OLLAMA_MODELS = {
    'fast': 'mistral',
    'standard': 'neural-chat',
    'powerful': 'llama2-uncensored',
}

# Workflow configuration
SYSTEM_CONTEXT = """
${blueprint.baseKnowledge || blueprint.description}
"""

# SQLite database untuk persistence
DB_FILE = os.getenv('DB_FILE', 'workflow_execution.db')

# Timeout ve retry settings
REQUEST_TIMEOUT = int(os.getenv('REQUEST_TIMEOUT', '300'))  # 5 dakika
MAX_RETRIES = int(os.getenv('MAX_RETRIES', '3'))
RETRY_BACKOFF_BASE = 2  # exponential backoff

# ================== DATABASE SETUP ==================

def init_database():
    """SQLite database'i olu≈ütur"""
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS executions (
            id TEXT PRIMARY KEY,
            blueprint_name TEXT,
            started_at TIMESTAMP,
            finished_at TIMESTAMP,
            status TEXT,
            total_time_ms INTEGER,
            node_count INTEGER,
            error_message TEXT
        )
    ''')
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS node_results (
            execution_id TEXT,
            node_id TEXT,
            node_title TEXT,
            started_at TIMESTAMP,
            finished_at TIMESTAMP,
            status TEXT,
            duration_ms INTEGER,
            output TEXT,
            error TEXT,
            FOREIGN KEY (execution_id) REFERENCES executions(id)
        )
    ''')
    
    conn.commit()
    conn.close()

# ================== HUGGINGFACE API ==================

def call_hf_with_retry(
    prompt: str,
    model: str = 'mistralai/Mistral-7B-Instruct-v0.2',
    max_retries: int = MAX_RETRIES
) -> tuple[bool, str, str]:
    """
    HuggingFace API'yi √ßaƒüƒ±r - retry logic ile
    Returns: (success, output, error)
    """
    
    if not HF_TOKEN:
        return False, '', 'HUGGINGFACE_TOKEN bulunamadƒ±'
    
    url = 'https://router.huggingface.co/v1/chat/completions'
    headers = {
        'Authorization': f'Bearer {HF_TOKEN}',
        'Content-Type': 'application/json',
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.post(
                url,
                headers=headers,
                json={
                    'model': model,
                    'messages': [
                        {'role': 'user', 'content': prompt}
                    ],
                    'max_tokens': 512,
                    'temperature': 0.7,
                    'stream': False
                },
                timeout=REQUEST_TIMEOUT
            )
            
            # Model y√ºkleniyor?
            if response.status_code == 503:
                error_text = response.text
                if 'loading' in error_text.lower():
                    wait_time = min(30 * (2 ** attempt), 60)  # max 60 saniye
                    print(f"‚è≥ Model y√ºkleniyor, {wait_time}s bekleniyor... (attempt {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
            
            # Rate limit?
            if response.status_code == 429:
                wait_time = min(5 * (2 ** attempt), 60)
                print(f"‚è≥ Rate limited, {wait_time}s bekleniyor... (attempt {attempt + 1}/{max_retries})")
                time.sleep(wait_time)
                continue
            
            # Ba≈üarƒ±lƒ±?
            if response.status_code == 200:
                data = response.json()
                # Chat Completions format parsing
                if 'choices' in data and len(data['choices']) > 0:
                    output = data['choices'][0].get('message', {}).get('content', '')
                elif isinstance(data, list) and len(data) > 0:
                    output = data[0].get('generated_text', '')
                else:
                    output = data.get('generated_text', '')
                return True, str(output).strip(), ''
            
            # Ciddi hata
            return False, '', f'API Error {response.status_code}: {response.text[:100]}'
        
        except requests.Timeout:
            wait_time = 2 ** attempt
            print(f"‚è≥ Timeout, {wait_time}s sonra retry... (attempt {attempt + 1}/{max_retries})")
            time.sleep(wait_time)
            continue
        
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt
                print(f"‚ö†Ô∏è Hata: {e}, {wait_time}s sonra retry...")
                time.sleep(wait_time)
                continue
            return False, '', str(e)
    
    return False, '', f'Max retries ({max_retries}) exceeded'

# ================== OLLAMA LOCAL ==================

def call_ollama(
    prompt: str,
    model: str = 'mistral'
) -> tuple[bool, str, str]:
    """Ollama local model'i √ßaƒüƒ±r"""
    
    try:
        response = requests.post(
            f'{OLLAMA_URL}/api/generate',
            json={
                'model': model,
                'prompt': prompt,
                'stream': False,
                'temperature': 0.7,
            },
            timeout=REQUEST_TIMEOUT
        )
        
        if response.status_code == 200:
            data = response.json()
            return True, data.get('response', ''), ''
        else:
            return False, '', f'Ollama error: {response.status_code}'
    
    except Exception as e:
        return False, '', f'Ollama connection failed: {e}'

# ================== UNIFIED API CALL ==================

def call_model(prompt: str, model_type: str = 'text') -> tuple[bool, str, str]:
    """
    Model'i √ßaƒüƒ±r - HF veya Ollama
    √ñnce tercih edilen sistem, ba≈üarƒ±sƒ±z olursa fallback
    """
    
    if USE_OLLAMA:
        print("[OLLAMA] √áalƒ±≈üƒ±yor...")
        success, output, error = call_ollama(prompt, OLLAMA_MODELS.get('fast', 'mistral'))
        if success:
            return True, output, ''
        print(f"[OLLAMA] Ba≈üarƒ±sƒ±z: {error}, HF'ye fallback...")
    
    # HuggingFace API'yi dene
    print("[HF] √áalƒ±≈üƒ±yor...")
    success, output, error = call_hf_with_retry(prompt, HF_MODELS.get(model_type, HF_MODELS['text']))
    return success, output, error

# ================== WORKFLOW NODES ==================

${blueprint.nodes.map((node, idx) => `
def node_${idx}_${node.id.replace(/-/g, '_')}(input_data: str) -> tuple[bool, str]:
    """
    Node ${idx + 1}: ${node.title}
    Tip: ${node.type}
    Rol: ${node.role}
    """
    
    print(f"\\n[Node ${idx}] üöÄ ${node.title} √ßalƒ±≈üƒ±yor...")
    start_time = time.time()
    
    prompt = f"""You are a ${node.role}.

Task: ${node.task}

Context: {SYSTEM_CONTEXT}

Input: {input_data}

Provide actionable output only."""
    
    try:
        success, output, error = call_model(prompt, 'text')
        
        if not success:
            print(f"[Node ${idx}] ‚ùå HATA: {error}")
            return False, f"Error: {error}"
        
        print(f"[Node ${idx}] ‚úÖ Tamamlandƒ± ({time.time() - start_time:.1f}s)")
        return True, output
    
    except Exception as e:
        print(f"[Node ${idx}] ‚ùå Exception: {e}")
        return False, f"Exception: {e}"
`).join('\n')}

# ================== MAIN EXECUTION ==================

def run_workflow(initial_input: str = "Start") -> Dict[str, Any]:
    """
    Workflow'u √ßalƒ±≈ütƒ±r - persistence ile
    """
    
    execution_id = f"{datetime.now().isoformat()}-{os.getenv('HOSTNAME', 'local')}"
    
    print("=" * 60)
    print(f"üè≠ ${blueprint.name}")
    print(f"üÜî Execution ID: {execution_id}")
    print("=" * 60)
    
    start_time = time.time()
    
    # Database kayƒ±t
    init_database()
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()
    
    cursor.execute('''
        INSERT INTO executions (id, blueprint_name, started_at, status, node_count)
        VALUES (?, ?, ?, ?, ?)
    ''', (execution_id, '${blueprint.name}', datetime.now(), 'running', ${blueprint.nodes.length}))
    conn.commit()
    
    results = {}
    current_output = initial_input
    failed = False
    
${blueprint.nodes.map((node, idx) => `    # ==================== Step ${idx + 1}: ${node.title} ====================
    node_start = time.time()
    success, output = node_${idx}_${node.id.replace(/-/g, '_')}(current_output)
    node_duration = int((time.time() - node_start) * 1000)
    
    if success:
        results['${node.id}'] = output
        current_output = output
        
        cursor.execute('''
            INSERT INTO node_results (execution_id, node_id, node_title, started_at, finished_at, status, duration_ms, output)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (execution_id, '${node.id}', '${node.title}', datetime.now(), datetime.now(), 'success', node_duration, output[:500]))
        conn.commit()
        print(f"[Node ${idx}] Output: {output[:100]}...")
    else:
        results['${node.id}'] = f"Error: {output}"
        
        cursor.execute('''
            INSERT INTO node_results (execution_id, node_id, node_title, status, duration_ms, error)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (execution_id, '${node.id}', '${node.title}', 'failed', node_duration, output[:200]))
        conn.commit()
        
        print(f"[Node ${idx}] ‚ùå BA≈ûARISIZ: {output}")
        failed = True
        break
    
    time.sleep(1)  # Rate limiting
`).join('\n')}
    
    # ==================== EXECUTION COMPLETE ====================
    
    total_time = int((time.time() - start_time) * 1000)
    status = 'failed' if failed else 'completed'
    
    cursor.execute('''
        UPDATE executions
        SET finished_at = ?, status = ?, total_time_ms = ?
        WHERE id = ?
    ''', (datetime.now(), status, total_time, execution_id))
    conn.commit()
    conn.close()
    
    print("\\n" + "=" * 60)
    if failed:
        print("‚ùå WORKFLOW BA≈ûARISIZ")
    else:
        print(f"‚úÖ WORKFLOW TAMAMLANDI ({total_time}ms)")
    print("=" * 60)
    
    return {
        'execution_id': execution_id,
        'status': status,
        'total_time_ms': total_time,
        'results': results,
        'db_file': DB_FILE
    }

# ================== ENTRY POINT ==================

if __name__ == "__main__":
    initial_input = sys.argv[1] if len(sys.argv) > 1 else "Workflow tetiklendi"
    
    results = run_workflow(initial_input)
    
    # Save results to JSON
    with open('workflow_results.json', 'w', encoding='utf-8') as f:
        json.dump({
            'execution_id': results['execution_id'],
            'status': results['status'],
            'total_time_ms': results['total_time_ms'],
            'results': results['results'],
            'db_file': results['db_file'],
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\\nüìÑ Sonu√ßlar kaydedildi: workflow_results.json")
    print(f"üìä Veriler saklandƒ±: {DB_FILE}")
    
    # Exit dengan status
    sys.exit(0 if results['status'] == 'completed' else 1)
`;

  const requirements = `# Requirements for ${blueprint.name}
# HuggingFace Free API + Ollama support
requests>=2.31.0
python-dotenv>=1.0.0

# Optional: Ollama client
# ollama>=0.1.0

# Database
# (built-in sqlite3)
`;

  const envExample = `# HuggingFace Configuration (0 maliyet)
HUGGINGFACE_TOKEN=your_free_hf_token_here
# Get free token from: https://huggingface.co/settings/tokens

# Ollama Configuration (local, completely free)
USE_OLLAMA=false
OLLAMA_URL=http://localhost:11434

# Execution Settings
REQUEST_TIMEOUT=300
MAX_RETRIES=3

# Database
DB_FILE=workflow_execution.db
`;

  const dockerFile = `FROM python:3.11-slim

WORKDIR /app

COPY main.py requirements.txt ./

RUN pip install --no-cache-dir -r requirements.txt

# Ollama optional
# RUN pip install ollama

ENV USE_OLLAMA=false
ENV HUGGINGFACE_TOKEN=""
ENV REQUEST_TIMEOUT=300

CMD ["python", "main.py"]
`;

  const dockerCompose = `version: '3.8'

services:
  workflow:
    build: .
    environment:
      HUGGINGFACE_TOKEN: \${HUGGINGFACE_TOKEN}
      USE_OLLAMA: false
      REQUEST_TIMEOUT: 300
    volumes:
      - ./workflow_results.json:/app/workflow_results.json
      - ./workflow_execution.db:/app/workflow_execution.db
    restart: on-failure

  # Optional: Ollama for local models (completely free)
  # ollama:
  #   image: ollama/ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama:/root/.ollama
  #   environment:
  #     OLLAMA_HOST: 0.0.0.0:11434

# volumes:
#   ollama:
`;

  return [
    { filename: 'main.py', content: mainScript, language: 'python' },
    { filename: 'requirements.txt', content: requirements, language: 'text' },
    { filename: '.env.example', content: envExample, language: 'text' },
    { filename: 'Dockerfile', content: dockerFile, language: 'docker' },
    { filename: 'docker-compose.yml', content: dockerCompose, language: 'yaml' },
  ];
};

// ==================== NODE.JS GENERATOR ====================

export const generateNodeJsScript = (blueprint: SystemBlueprint): GeneratedCode[] => {
  const mainScript = `#!/usr/bin/env node
/**
 * ${blueprint.name}
 * Auto-generated by OmniFlow Factory - HuggingFace Native
 * ‚úÖ 0 maliyetli (Free HF API + Ollama)
 */

const http = require('http');
const https = require('https');
const { promisify } = require('util');
const fs = require('fs').promises;
const sqlite3 = require('sqlite3').verbose();
const path = require('path');

// ==================== CONFIG ====================

const USE_OLLAMA = process.env.USE_OLLAMA === 'true';
const HF_TOKEN = process.env.HUGGINGFACE_TOKEN || '';
const OLLAMA_URL = process.env.OLLAMA_URL || 'http://localhost:11434';
const REQUEST_TIMEOUT = parseInt(process.env.REQUEST_TIMEOUT || '300') * 1000;
const MAX_RETRIES = parseInt(process.env.MAX_RETRIES || '3');
const DB_FILE = process.env.DB_FILE || 'workflow_execution.db';

const SYSTEM_CONTEXT = \`${blueprint.baseKnowledge || blueprint.description}\`;

// ==================== DATABASE ====================

function initDatabase() {
  return new Promise((resolve, reject) => {
    const db = new sqlite3.Database(DB_FILE, (err) => {
      if (err) reject(err);
      
      db.serialize(() => {
        db.run(\`
          CREATE TABLE IF NOT EXISTS executions (
            id TEXT PRIMARY KEY,
            blueprint_name TEXT,
            started_at DATETIME,
            finished_at DATETIME,
            status TEXT,
            total_time_ms INTEGER,
            node_count INTEGER,
            error_message TEXT
          )
        \`);
        
        db.run(\`
          CREATE TABLE IF NOT EXISTS node_results (
            execution_id TEXT,
            node_id TEXT,
            node_title TEXT,
            started_at DATETIME,
            finished_at DATETIME,
            status TEXT,
            duration_ms INTEGER,
            output TEXT,
            error TEXT,
            FOREIGN KEY (execution_id) REFERENCES executions(id)
          )
        \`, (err) => {
          if (err) reject(err);
          else resolve(db);
        });
      });
    });
  });
}

// ==================== HTTP HELPERS ====================

function post(url, data, timeout = REQUEST_TIMEOUT) {
  return new Promise((resolve, reject) => {
    const client = url.startsWith('https') ? https : http;
    const options = new URL(url);
    
    options.method = 'POST';
    options.timeout = timeout;
    options.headers = {
      'Content-Type': 'application/json',
      'Content-Length': Buffer.byteLength(data)
    };
    
    const req = client.request(options, (res) => {
      let body = '';
      res.on('data', chunk => body += chunk);
      res.on('end', () => resolve({ status: res.statusCode, body }));
    });
    
    req.on('timeout', () => {
      req.destroy();
      reject(new Error('Request timeout'));
    });
    
    req.on('error', reject);
    req.write(data);
    req.end();
  });
}

// ==================== HUGGINGFACE API ====================

async function callHFWithRetry(prompt, model = 'mistralai/Mistral-7B-Instruct-v0.2') {
  if (!HF_TOKEN) {
    return { success: false, output: '', error: 'HUGGINGFACE_TOKEN not found' };
  }
  
  const url = 'https://router.huggingface.co/v1/chat/completions';
  
  for (let attempt = 0; attempt < MAX_RETRIES; attempt++) {
    try {
      const data = JSON.stringify({
        model: model,
        messages: [
          { role: 'user', content: prompt }
        ],
        max_tokens: 512,
        temperature: 0.7,
        stream: false
      });
      
      const res = await post(url, data);
      
      if (res.status === 200) {
        const parsed = JSON.parse(res.body);
        let output = '';
        if (parsed.choices && parsed.choices.length > 0) {
          output = parsed.choices[0].message?.content || '';
        } else if (Array.isArray(parsed)) {
          output = parsed[0]?.generated_text || '';
        } else {
          output = parsed?.generated_text || '';
        }
        return { success: true, output: String(output).trim(), error: '' };
      }
      
      if (res.status === 503 && res.body.includes('loading')) {
        const wait = Math.min(30 * Math.pow(2, attempt), 60000);
        console.log(\`‚è≥ Model loading, waiting \${wait}ms...\`);
        await new Promise(r => setTimeout(r, wait));
        continue;
      }
      
      if (res.status === 429) {
        const wait = Math.min(5000 * Math.pow(2, attempt), 60000);
        console.log(\`‚è≥ Rate limited, waiting \${wait}ms...\`);
        await new Promise(r => setTimeout(r, wait));
        continue;
      }
      
      return { success: false, output: '', error: \`API Error \${res.status}\` };
    } catch (error) {
      if (attempt < MAX_RETRIES - 1) {
        const wait = Math.pow(2, attempt) * 1000;
        console.log(\`‚ö†Ô∏è Retry in \${wait}ms...\`);
        await new Promise(r => setTimeout(r, wait));
      } else {
        return { success: false, output: '', error: String(error) };
      }
    }
  }
  
  return { success: false, output: '', error: 'Max retries exceeded' };
}

// ==================== WORKFLOW NODES ==================

${blueprint.nodes.map((node, idx) => `
async function node_${idx}_${node.id.replace(/-/g, '_')}(inputData) {
  console.log(\`\\n[Node ${idx}] üöÄ ${node.title} starting...\`);
  
  const prompt = \`You are a ${node.role}.
Task: ${node.task}
Context: \${SYSTEM_CONTEXT}
Input: \${inputData}

Provide actionable output only.\`;
  
  const result = await callHFWithRetry(prompt, 'mistralai/Mistral-7B-Instruct-v0.2');
  
  if (!result.success) {
    console.log(\`[Node ${idx}] ‚ùå FAILED: \${result.error}\`);
    return { success: false, output: \`Error: \${result.error}\` };
  }
  
  console.log(\`[Node ${idx}] ‚úÖ Completed\`);
  return { success: true, output: result.output };
}
`).join('\n')}

// ==================== MAIN ====================

async function runWorkflow(initialInput = 'Start') {
  console.log('=' .repeat(60));
  console.log(\`üè≠ ${blueprint.name}\`);
  console.log('=' .repeat(60));
  
  const db = await initDatabase();
  const executionId = \`\${new Date().toISOString()}-\${require('os').hostname()}\`;
  const startTime = Date.now();
  
  const dbRun = promisify(db.run.bind(db));
  
  await dbRun(
    'INSERT INTO executions (id, blueprint_name, started_at, status, node_count) VALUES (?, ?, ?, ?, ?)',
    [executionId, '${blueprint.name}', new Date(), 'running', ${blueprint.nodes.length}]
  );
  
  let currentOutput = initialInput;
  const results = {};
  let failed = false;

${blueprint.nodes.map((node, idx) => `  // Step ${idx + 1}: ${node.title}
  const nodeStart = Date.now();
  const step${idx} = await node_${idx}_${node.id.replace(/-/g, '_')}(currentOutput);
  const nodeDuration = Date.now() - nodeStart;
  
  if (step${idx}.success) {
    results['${node.id}'] = step${idx}.output;
    currentOutput = step${idx}.output;
    
    await dbRun(
      'INSERT INTO node_results (execution_id, node_id, node_title, status, duration_ms, output) VALUES (?, ?, ?, ?, ?, ?)',
      [executionId, '${node.id}', '${node.title}', 'success', nodeDuration, step${idx}.output.substring(0, 500)]
    );
  } else {
    results['${node.id}'] = step${idx}.output;
    
    await dbRun(
      'INSERT INTO node_results (execution_id, node_id, node_title, status, duration_ms, error) VALUES (?, ?, ?, ?, ?, ?)',
      [executionId, '${node.id}', '${node.title}', 'failed', nodeDuration, step${idx}.output.substring(0, 200)]
    );
    
    failed = true;
    break;
  }
  
  await new Promise(r => setTimeout(r, 1000)); // Rate limiting
`).join('\n')}
  
  const totalTime = Date.now() - startTime;
  const status = failed ? 'failed' : 'completed';
  
  await dbRun(
    'UPDATE executions SET finished_at = ?, status = ?, total_time_ms = ? WHERE id = ?',
    [new Date(), status, totalTime, executionId]
  );
  
  console.log('\\n' + '=' .repeat(60));
  console.log(failed ? '‚ùå WORKFLOW FAILED' : \`‚úÖ WORKFLOW COMPLETED (\${totalTime}ms)\`);
  console.log('=' .repeat(60));
  
  db.close();
  
  // Save results
  await fs.writeFile('workflow_results.json', JSON.stringify({
    executionId,
    status,
    totalTime,
    results,
    dbFile: DB_FILE
  }, null, 2));
  
  console.log('\\nüìÑ Results saved to workflow_results.json');
  process.exit(failed ? 1 : 0);
}

runWorkflow().catch(err => {
  console.error('‚ùå Fatal error:', err);
  process.exit(1);
});
`;

  const packageJson = {
    name: blueprint.name.toLowerCase().replace(/\\s+/g, '-'),
    version: '1.0.0',
    description: `Auto-generated by OmniFlow - ${blueprint.description}`,
    main: 'main.js',
    scripts: {
      start: 'node main.js',
      test: 'node main.js "Test input"'
    },
    dependencies: {
      sqlite3: '^5.1.6'
    },
    engines: {
      node: '>=14.0.0'
    }
  };

  const envExample = `# HuggingFace (Free)
HUGGINGFACE_TOKEN=your_free_hf_token_here

# Ollama (Local, Free)
USE_OLLAMA=false
OLLAMA_URL=http://localhost:11434

# Settings
REQUEST_TIMEOUT=300
MAX_RETRIES=3
DB_FILE=workflow_execution.db
`;

  return [
    { filename: 'main.js', content: mainScript, language: 'javascript' },
    { filename: 'package.json', content: JSON.stringify(packageJson, null, 2), language: 'json' },
    { filename: '.env.example', content: envExample, language: 'text' },
  ];
};

// ==================== GITHUB ACTION GENERATOR ====================

export const generateGitHubAction = (blueprint: SystemBlueprint): GeneratedCode[] => {
  const workflow = `.github/workflows/${blueprint.name.toLowerCase().replace(/\\s+/g, '-')}.yml`;

  const content = `name: ${blueprint.name}

on:
  workflow_dispatch:
    inputs:
      input:
        description: 'Workflow input'
        required: false
        default: 'GitHub Action triggered'
  schedule:
    # Her 6 saatte bir √ßalƒ±≈ü (istenirse deƒüi≈ütir)
    - cron: '0 */6 * * *'

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run workflow
        env:
          HUGGINGFACE_TOKEN: \${{ secrets.HUGGINGFACE_TOKEN }}
          REQUEST_TIMEOUT: '300'
          MAX_RETRIES: '3'
        run: python main.py "\${{ github.event.inputs.input || 'GitHub Action scheduled run' }}"
      
      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: workflow-results
          path: |
            workflow_results.json
            workflow_execution.db
      
      - name: Send notification
        if: always()
        run: |
          STATUS="\${{ job.status }}"
          echo "Workflow \$STATUS"
          # Optional: Send to Slack, Discord, Telegram, etc.
`;

  return [
    { filename: workflow, content, language: 'yaml' }
  ];
};

// ==================== EXPORT FUNCTION ====================

export const generateCode = (blueprint: SystemBlueprint, format: ExportFormat): GeneratedCode[] => {
  switch (format) {
    case 'python':
      return generatePythonScript(blueprint);
    case 'nodejs':
      return generateNodeJsScript(blueprint);
    case 'github-action':
      return generateGitHubAction(blueprint);
    default:
      return generatePythonScript(blueprint);
  }
};
